{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6He7Vb3lGBE",
        "outputId": "0fb0a920-e355-4bcb-f30f-7c62cc17048c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LreozloM6RYSZ9tBNAv0acWB6Umb4NrX\n",
            "To: /content/agatha_christie.zip\n",
            "100%|██████████| 1.85M/1.85M [00:00<00:00, 195MB/s]\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1LreozloM6RYSZ9tBNAv0acWB6Umb4NrX'\n",
        "\n",
        "output = 'agatha_christie.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "!unzip -q agatha_christie.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FQQCSOQBSxTb"
      },
      "outputs": [],
      "source": [
        "path = './agatha_christie/'\n",
        "\n",
        "text_1 = open(path + 'assassinato_no_expresso_oriente.txt', 'r').read()\n",
        "text_2 = open(path + 'contos_de_miss_marple.txt', 'r').read()\n",
        "text_3 = open(path + 'e_nao_sobrou_nenhum.txt', 'r').read()\n",
        "text_4 = open(path + 'e_no_final_a_morte.txt', 'r').read()\n",
        "text_5 = open(path + 'morte_no_nilo.txt', 'r').read()\n",
        "text_6 = open(path + 'nemesis.txt', 'r').read()\n",
        "text_7 = open(path + 'o_assassinato_de_roger_ackroyd.txt', 'r').read()\n",
        "text_8 = open(path + 'o_clube_das_tercas_feiras.txt', 'r').read()\n",
        "text_9 = open(path + 'os_crimes_abc.txt', 'r').read()\n",
        "text_10 = open(path + 'poirot_investiga.txt', 'r').read()\n",
        "text_11 = open(path + 'retrato_inacabado.txt', 'r').read()\n",
        "text_12 = open(path + 'um_corpo_na_biblioteca.txt', 'r').read()\n",
        "text_13 = open(path + 'um_destino_ignorado.txt', 'r').read()\n",
        "text_14 = open(path + 'uma_dose_mortal.txt', 'r').read()\n",
        "\n",
        "\n",
        "text = text_1 + text_2 + text_3 + text_4 + text_5 + text_6 + text_7 + text_8 + text_9 + text_10 + text_11 + text_12 + text_13 + text_14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fFmqBRuATlhd"
      },
      "outputs": [],
      "source": [
        "agatha_christie = open('agatha_christie.txt', 'w')\n",
        "agatha_christie.write(text)\n",
        "agatha_christie.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('agatha_christie.txt', 'r').read()\n",
        "vocab = sorted(set(text))"
      ],
      "metadata": {
        "id": "B8egfSJdw2y4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dCjhWtqRTwQf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "char_to_ind = { char:ind for ind, char in enumerate(vocab) }\n",
        "ind_to_char = np.array(vocab)\n",
        "encoded_text = np.array([ char_to_ind[c] for c in text ])\n",
        "seq_len = 120\n",
        "total_num_seq = len(text)//(seq_len + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sHN_YgoDUIdA"
      },
      "outputs": [],
      "source": [
        "def create_seq_targets(seq):\n",
        "    input_txt = seq[:-1]\n",
        "    target_txt = seq[1:]\n",
        "    return input_txt, target_txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0Y_ClrSbUKar"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "sequences = char_dataset.batch(seq_len + 1, drop_remainder=True)\n",
        "\n",
        "dataset = sequences.map(create_seq_targets)\n",
        "\n",
        "batch_size = 128\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wGi3a6doaIFk"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Embedding,GRU\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UzWhN10AUlVm"
      },
      "outputs": [],
      "source": [
        "def sparse_cat_loss(y_true, y_pred):\n",
        "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "\n",
        "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\n",
        "    model.add(GRU(rnn_neurons,return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pz6DU4tzUn7q"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "embed_dim = 64\n",
        "rnn_neurons = 1026"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3fvbA_wHU-W5"
      },
      "outputs": [],
      "source": [
        "model = create_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embed_dim = embed_dim,\n",
        "  rnn_neurons = rnn_neurons,\n",
        "  batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CsvpRPEyVCx6"
      },
      "outputs": [],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "T-OS2gU1xCRd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30"
      ],
      "metadata": {
        "id": "QQuA9l5DxDF8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhhqZvHRVIYd",
        "outputId": "79171ce3-1ad5-4af4-9f59-7b428f61e5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "312/312 [==============================] - 45s 127ms/step - loss: 2.5964\n",
            "Epoch 2/30\n",
            "312/312 [==============================] - 41s 123ms/step - loss: 1.8625\n",
            "Epoch 3/30\n",
            "312/312 [==============================] - 42s 127ms/step - loss: 1.4970\n",
            "Epoch 4/30\n",
            "312/312 [==============================] - 40s 123ms/step - loss: 1.3388\n",
            "Epoch 5/30\n",
            "312/312 [==============================] - 40s 123ms/step - loss: 1.2615\n",
            "Epoch 6/30\n",
            "312/312 [==============================] - 42s 125ms/step - loss: 1.2126\n",
            "Epoch 7/30\n",
            "312/312 [==============================] - 43s 132ms/step - loss: 1.1776\n",
            "Epoch 8/30\n",
            "312/312 [==============================] - 42s 129ms/step - loss: 1.1498\n",
            "Epoch 9/30\n",
            "312/312 [==============================] - 43s 131ms/step - loss: 1.1273\n",
            "Epoch 10/30\n",
            "312/312 [==============================] - 41s 123ms/step - loss: 1.1079\n",
            "Epoch 11/30\n",
            "312/312 [==============================] - 42s 128ms/step - loss: 1.0904\n",
            "Epoch 12/30\n",
            "312/312 [==============================] - 43s 133ms/step - loss: 1.0749\n",
            "Epoch 13/30\n",
            "312/312 [==============================] - 42s 129ms/step - loss: 1.0602\n",
            "Epoch 14/30\n",
            "312/312 [==============================] - 41s 126ms/step - loss: 1.0468\n",
            "Epoch 15/30\n",
            "312/312 [==============================] - 42s 124ms/step - loss: 1.0343\n",
            "Epoch 16/30\n",
            "312/312 [==============================] - 43s 131ms/step - loss: 1.0229\n",
            "Epoch 17/30\n",
            "312/312 [==============================] - 43s 130ms/step - loss: 1.0117\n",
            "Epoch 18/30\n",
            "312/312 [==============================] - 43s 131ms/step - loss: 1.0016\n",
            "Epoch 19/30\n",
            "312/312 [==============================] - 43s 130ms/step - loss: 0.9922\n",
            "Epoch 20/30\n",
            "312/312 [==============================] - 43s 131ms/step - loss: 0.9837\n",
            "Epoch 21/30\n",
            "312/312 [==============================] - 42s 129ms/step - loss: 0.9757\n",
            "Epoch 22/30\n",
            "312/312 [==============================] - 41s 125ms/step - loss: 0.9685\n",
            "Epoch 23/30\n",
            "312/312 [==============================] - 43s 132ms/step - loss: 0.9619\n",
            "Epoch 24/30\n",
            "312/312 [==============================] - 44s 129ms/step - loss: 0.9555\n",
            "Epoch 25/30\n",
            "312/312 [==============================] - 43s 130ms/step - loss: 0.9499\n",
            "Epoch 26/30\n",
            "312/312 [==============================] - 41s 125ms/step - loss: 0.9450\n",
            "Epoch 27/30\n",
            "312/312 [==============================] - 43s 132ms/step - loss: 0.9408\n",
            "Epoch 28/30\n",
            "312/312 [==============================] - 42s 129ms/step - loss: 0.9369\n",
            "Epoch 29/30\n",
            "312/312 [==============================] - 40s 122ms/step - loss: 0.9336\n",
            "Epoch 30/30\n",
            "312/312 [==============================] - 41s 126ms/step - loss: 0.9307\n"
          ]
        }
      ],
      "source": [
        "model.fit(dataset, epochs=epochs)\n",
        "model.save('agatha_christie_gen.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-tMr_DRWVTFW"
      },
      "outputs": [],
      "source": [
        "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
        "model.load_weights('agatha_christie_gen.h5')\n",
        "model.build(tf.TensorShape([1, None]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2ZzKc39PVhFl"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_seed, gen_size=100, temp=1.0):\n",
        "    num_generate = gen_size\n",
        "\n",
        "    input_eval = [char_to_ind[s] for s in start_seed]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    temperature = temp\n",
        "\n",
        "    model.reset_states()\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(ind_to_char[predicted_id])\n",
        "\n",
        "    return (start_seed + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FsPtwJPVm1T",
        "outputId": "18505399-63bc-4601-a2b0-c9a47c58b62b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poirot. — Acha que eu acredito nisso... Você quer dizer. Um homem muito agrestivo.\n",
            "      — O quê? — dizia ela. — Mas se não está falando,\n",
            "nha. A mancada de como eno marido, de modo e percepeu de ouvimonalidades postatos e, no meio da frente. Poirot não achou a mesma cigada grave, sob uma porta ágna — Penversieu que foi afirmando qualquer coisa — retrucou Poirot. — E você readman brilharam de Caroline a Bendida. Eu teria que quisem...\n",
            "\n",
            "Ela embôlicatone, Piara apenas um sujeito removia com a perspectiva que entrou em condições de repente ele escrevera no papéis de Arden e enrolava-se aos quartos mais rixaim destruídas.\n",
            "\n",
            "Ela veio até o motorista, um tipo de aspiros. Encontrou a recita de quem diria que ele daínca era inorente de Frank e dois corpo perigoso.\n",
            "\n",
            "\n",
            "\n",
            "AII\n",
            "\n",
            "\t\t\t“‘Está tão absolutamente InIgntur a opinião pública era dura. Como sair, Jacqueline. Não percebia o tempo todo o tempo todo sabera.\n",
            "\n",
            "Sua mente dele tentou amavam a mal. Mas Célia compraram gordo.\n",
            "\n",
            "— Concordo com intenção, a senhor\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(model, \"Poirot\", gen_size=1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8PdslbtHQdG",
        "outputId": "52de81f6-4c15-47dd-82b9-dd5eff969b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Morte conservado uma grande bela de um detalhe amigo Sir Henry dos Margueritos. Quando pudermos tomar nossas lucro tanto melancro disse que Sir Cuém a batir de todos o que Paton. Pode-se de mim. Quando me fiz. Dentro em voz alta, domingo poderia ter atirado em Londres e o coronel Bantry não está encarregado de concordar com M.\n",
            "\n",
            "– São obsesve e tudo, em sua mãe no escritório de Dartor aos dois antigas antes morreram na aldeia. A mãe era meio comunista. Suas dois sempre fazem amíveixades ferroviária tendo sido alguém nunca recuso. Desoa devido ao acrasa para fora, estava passando de um olhar fixo de livro para casa. Quando já esperava, por sua vez, mas me assustou-se, pensou termonou uma companhia discreta, que confessou alguma coisa sobre a mente:\n",
            "\n",
            "— Mary Dabem, monsieur Poirot — gouve o problema com tinta invadiável, mas três vezes junto com a polícia um dia fora útil nas entristez do mundo... precisamos explicá-lo do sr. Morley. Ele estivera esperando, teria a gorjeta de certo modo? Não co\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(model, \"Morte\", gen_size=1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJIBg0_nAkko",
        "outputId": "11fa2f73-4ef6-4fad-a4ed-b7eb6df32fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quarto Amarelo que as menores seremos bem\n",
            "verdadeiros, mas sem provas.\n",
            "\n",
            "“É por surdo em seu letal, elas não reparei que entendamos a srta. Sainsbury Seale, que o senhor atendia fazer detalhes meus vitos reais da Inglaterra. Aírei irritada.\n",
            "\n",
            "– A senhorita pudisse, teremos mais nada dele...\n",
            "\n",
            "Poirot gentilze ansiosós real. No momento, eu dava meias rapidamente, quase que cuidadosa pela menina quase desconchad...\n",
            "\n",
            "Ursula moço diretor da loja da Sra. Ascher, e amigos os anos, difícil dar. Peter me veio para Davenheim.\n",
            "\n",
            "— Ah, isso, claro — disse BCulpes — limpou o gatela de costume, o inspetor, que ficava narrativa com a bandeja.\n",
            "\n",
            "– Mas o senhor fica a resposta. – Mas é fácil eu estava. Já obviou, pensativa.\n",
            "\n",
            "Poirot disse:\n",
            "\n",
            "– Derrubou as mãos.\n",
            "\n",
            "– Não mexericou a srta. Morley na mas de distância.\n",
            "\n",
            "— Quero que a senhorita se lembra de teruo normal. Britânico na casa, confiante, discrettaram-se bruscamente, amigos para observá-lo de vez em quando. Estou corto de quem contratara o nome de Tim hospitais.\n",
            "\n",
            "Conc\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(model, \"Quarto Amarelo\", gen_size=1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye0UybLSAqRJ",
        "outputId": "6a28b665-fc50-4ff3-8fdf-12addee7bc71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nilouxar outra loucura. Estava embolados de lã muro.\n",
            "\n",
            "– Pois bem, que lado nome – Joce disse ao mar, Cornelea.\n",
            "\n",
            "– Será que ele traz ao ar da minha missão?\n",
            "\n",
            "O sr. Howard Raikes disse:\n",
            "\n",
            "– A senhora a tinha para si mesmo. O piano era escuro, encontrou a matada por Miriam, em torno da moça — sobro conforme do meu país, além de ser uma coisa que por acaso assim o quí. Pobreviveis! Ele queria que o senhor pense.\n",
            "\n",
            "Eles vão telefôncias, disseram que o sr. Jefferson teria, quero dizer. Os pais dela tenha se esgora natural. Esta é uma delicazendo de se sair. Tudo isso dirio que chamou Peter um vestido de libo. Lembre-se me disse que o detendesse baixando a voz inicial, sem necessito como a própria aventura. – E irritante do consultório? – indvolta – disse. – É uma coisa muito mais velha. Havia alguma costa quando essa sra. Sembra então neste lugar é preciso esperar uemos um jeito? Insistisse o deserto.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "II\n",
            "\n",
            "\n",
            "Tanto na vida era Majar Raymond Weslmer estar enganada, quando dirigentes. Mais a maneir\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(model, \"Nilo\", gen_size=1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIXNmJURHMxg",
        "outputId": "9f67de1e-3e12-4063-dabc-0fa578ab1cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assassinato, archur, e, no final, fizeram-na abafana tudo em você. Estou esperando um telegrama na direção de Ferro para cáleas diabóliciamos na sala. E agora me perguntei ao sol. Númera da sra vantagem de encantar a situação entreler vivo, e não deram. Somos importantes em nem cabo de amor setura. Disse:\n",
            "\n",
            "– Vou dizer, até ontem, sentada nele, senhor. Mas Henet só posso dizer. Mademoiselle se pondo a pensa que Miss Jeanne não o atordoalidão está certo. Olhou assustado para trás, eno mundo. Bastava-se começou a traram-se sobre a mesa do café da manhã.\n",
            "\n",
            "As lareneiras foram ali deitado, bem-moespero, insolente, até alfaberar. Dizia que início. Era isso. Como eu tracaram, diz que isso depende muito a carta como caridade o que quero dizer isso. Me ouviu falar de novo seus maridos, Lady Willard era aquela visita a deixar a ela e depois entraram. Tirou o dinheiro quando formou fogo.\n",
            "\n",
            "– Não tinha formado vindo de carro azul no quiosque?\"       Depois bateu profundamente malfeito. Então chegou ao bom, e e\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(model, \"Assassinato\", gen_size=1000))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}